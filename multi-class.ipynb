{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: dask in /usr/local/lib/python2.7/site-packages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import tensorflow as tf\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math \n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import tsa_utils\n",
    "import deep_cnn\n",
    "import mini_cnn\n",
    "\n",
    "from shutil import copytree\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import tensorflow as tf\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "import tsa_utils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import tensorflow as tf\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math \n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import tsa_utils\n",
    "import mini_cnn\n",
    "import pip\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import tensorflow as tf\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math \n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import tsa_utils\n",
    "import mini_cnn\n",
    "\n",
    "pip.main(['install', '--upgrade', 'dask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copytree(\"/models/multi-class8/multi_class\", \"/output/multi_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "BATCH_SIZE=10\n",
    "FILTER_COUNT=12\n",
    "KERNEL_SIZE1=(1,6,6)\n",
    "DEPTHSTRIDE=1\n",
    "XSTRIDE=1\n",
    "YSTRIDE=1\n",
    "POOLSIZE1=(16,3,3)\n",
    "POOLSIZE2=(3,3)\n",
    "POOL_STRIDES1=(1,2,2)\n",
    "POOL_STRIDES=(1,1)\n",
    "STEPS=3000\n",
    "XSIZE=392\n",
    "#XSIZE=512\n",
    "#XSIZE=270\n",
    "#YSIZE=340\n",
    "YSIZE=660\n",
    "LEARNING_RATE=0.01\n",
    "EPSILON=0\n",
    "IMAGE_DEPTH=16\n",
    "CHANNELS=1\n",
    "FLAT_POOL_SIZE=46656\n",
    "\n",
    "DATA_PATH=\"/data_volume/\"\n",
    "CHECKPOINT_PATH=\"/output/multi_class\"\n",
    "MODEL_ID=\"multi_class\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "slice_locations = {1:(\"top\",\"right\"),\n",
    "\t2:(\"middle\",\"right\"), \n",
    "\t3:(\"top\",\"left\"),\n",
    "\t4:(\"middle\",\"left\"),\n",
    "\t5:(\"top\",\"middle\"),\n",
    "\t6:(\"top\",\"right\"),\n",
    "\t7:(\"top\",\"left\"),\n",
    "\t8:(\"middle\",\"right\"),\n",
    "\t9:(\"middle\",\"middle\"),\n",
    "\t10:(\"middle\",\"left\"),\n",
    "\t11:(\"bottom\",\"right\"),\n",
    "\t12:(\"bottom\",\"left\"),\n",
    "\t13:(\"bottom\",\"right\"),\n",
    "\t14:(\"bottom\",\"left\"),\n",
    "\t15:(\"bottom\",\"right\"),\n",
    "\t16:(\"bottom\",\"left\"),\n",
    "\t17:(\"top\",\"middle\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ZoneModel():\n",
    "\n",
    "    def __init__(self, model_id, ids, x_slice, y_slice, data_path, labels, checkpoint_path=\".\", localize=False):\n",
    "        self.model_id = model_id\n",
    "        self.ids = ids\n",
    "        self.x_slice = x_slice\n",
    "        self.y_slice = y_slice\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.labels = labels\n",
    "        self.localize = localize\n",
    "\n",
    "    def build_model(self, data, labels, mode):\n",
    "        if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            BATCH_SIZE=1\n",
    "        else:\n",
    "            BATCH_SIZE=10\n",
    "        print(XSIZE, YSIZE)\n",
    "        data = tf.reshape(data, [BATCH_SIZE, IMAGE_DEPTH, YSIZE, XSIZE, CHANNELS])\n",
    "        conv1 = tf.layers.conv3d(inputs=data, filters=FILTER_COUNT, kernel_size=KERNEL_SIZE1, padding=\"same\", \n",
    "                strides=(DEPTHSTRIDE,XSTRIDE,YSTRIDE), name=\"conv1\", trainable=not self.localize)\n",
    "        print(conv1)\n",
    "        conv2 = tf.layers.conv3d(inputs=conv1, filters=FILTER_COUNT, kernel_size=(3,3,3), padding=\"same\",\n",
    "                strides=(1, 2, 2), name=\"conv2\", activation=tf.nn.relu, trainable=not self.localize)\n",
    "        print(conv2)\n",
    "        pool1 = tf.layers.max_pooling3d(inputs=conv2, pool_size=POOLSIZE1, strides=POOL_STRIDES1, name=\"pool1\")\n",
    "        print(pool1)\n",
    "        flattener = tf.reshape(pool1, [BATCH_SIZE, 164, 97, 12])\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=flattener, pool_size=(3,3), strides=(2,2), name=\"pool2\")\n",
    "        print(pool2)\n",
    "        conv3 = tf.layers.conv2d(inputs=pool2, filters=FILTER_COUNT, kernel_size=(3,3), padding=\"same\", \n",
    "                strides=(XSTRIDE,YSTRIDE), name=\"conv3\", trainable=not self.localize)\n",
    "        print(conv3)\n",
    "        pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=(2,2), strides=POOL_STRIDES, name=\"pool1\")\n",
    "        print(pool3)\n",
    "        conv4 = tf.layers.conv2d(inputs=pool3, filters=FILTER_COUNT, kernel_size=(3,3), padding=\"same\", \n",
    "                strides=(1, 1), name=\"conv4\", activation=tf.nn.relu, trainable=not self.localize)\n",
    "        print(conv4)\n",
    "        pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=POOLSIZE2, strides=POOL_STRIDES, name=\"pool2\", padding=\"same\")\n",
    "        print(pool4)\n",
    "        flat_pool = tf.reshape(pool2, [BATCH_SIZE, FLAT_POOL_SIZE])\n",
    "        flat_pool=tf.identity(flat_pool, name=\"flat_pool\")\n",
    "\n",
    "        logits = tf.layers.dense(inputs=flat_pool, units=17, trainable=True)\n",
    "        logits = tf.identity(logits, name=\"logits\")\n",
    "        logits = tf.reshape(logits, [BATCH_SIZE,17])\n",
    "        predictions = {\n",
    "            \"classes\": tf.argmax(\n",
    "              input=logits, axis=1, name=\"classes\"),\n",
    "          \"probabilities\": tf.nn.sigmoid(\n",
    "              logits, name=\"prob_tensor\")}\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            #flat_labels = tf.reshape(labels, [BATCH_SIZE, 17])\n",
    "            labels=tf.identity(labels, name=\"labels\")\n",
    "            #class_weights=tf.reduce_sum(tf.multiply(flat_labels, tf.constant(WEIGHTS, dtype=tf.int64)), axis=1)\n",
    "            #print(class_weights)\n",
    "            #loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "            loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=tf.contrib.framework.get_global_step(),\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                optimizer=\"SGD\")\n",
    "        if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            return tf.contrib.learn.ModelFnOps(mode=mode, predictions=predictions)\n",
    "        return tf.contrib.learn.ModelFnOps(mode=mode, predictions=predictions, loss=loss, train_op=train_op)\n",
    "\n",
    "    def train_model(self, tensors_to_log, reuse=False):\n",
    "        if reuse:\n",
    "            tsa_classifier = self.model\n",
    "        else:\n",
    "            tsa_classifier = tf.contrib.learn.Estimator(model_fn=self.build_model, \n",
    "                                                     model_dir=self.checkpoint_path + \"/\" + self.model_id)\n",
    "        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=100)\n",
    "        tsa_classifier.fit(\n",
    "            x=InputImagesIterator(self.ids, self.data_path, 10000, self.y_slice, self.x_slice), \n",
    "            y=InputLabelsIterator(self.ids, self.labels), \n",
    "            steps=STEPS, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            monitors=[logging_hook])\n",
    "        self.model = tsa_classifier\n",
    "\n",
    "    def load_model(self):\n",
    "        print(self.checkpoint_path, self.model_id)\n",
    "        tsa_classifier = tf.contrib.learn.Estimator(model_fn=self.build_model, \n",
    "                                                    model_dir=self.checkpoint_path + \"/\" + self.model_id)\n",
    "        print (\"LOADED MODEL AT STEP \" + str(tsa_classifier.get_variable_value(\"global_step\")) + \" FROM \" + self.checkpoint_path + \"/\" + self.model_id )\n",
    "        self.model = tsa_classifier\n",
    "\n",
    "    def bootstrap_model(self, path=None):\n",
    "        if path:\n",
    "            checkpoint_path = path\n",
    "        else:\n",
    "            checkpoint_path = self.checkpoint_path + \"/\" + self.model_id\n",
    "        print (\"USING CHECKPOINT PATH  \" + checkpoint_path)\n",
    "        tsa_classifier = tf.contrib.learn.Estimator(model_fn=self.build_model, \n",
    "                                                    model_dir=checkpoint_path)\n",
    "        print (\"BOOTSTRAPPED AT STEP \" + str(tsa_classifier.get_variable_value(\"global_step\")))\n",
    "        self.model = tsa_classifier \n",
    "\n",
    "    def predict(self):\n",
    "        return self.model.predict(x=InputImagesIterator(self.ids, \n",
    "                                                            self.data_path, \n",
    "                                                            10000, \n",
    "                                                            self.y_slice,\n",
    "                                                            self.x_slice, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_df = pd.read_csv(DATA_PATH + '/stage1_labels.csv')\n",
    "image_df['zone'] = image_df['Id'].str.split(\"_Zone\", expand=True)[1].str.strip()\n",
    "image_df['id'] = image_df['Id'].str.split(\"_\", expand=True)[0].str.strip()\n",
    "\n",
    "ids = image_df[\"id\"].unique()\n",
    "ids.sort()\n",
    "\n",
    "labels = image_df[image_df['id'].isin(ids)]\n",
    "labels = labels.sort_values(\"id\")\n",
    "\n",
    "tensors_to_log =  {\"probabilities\": \"prob_tensor\",\n",
    "                    \"actual\":\"labels\"}\n",
    "labels.zone = labels.zone.astype(\"int\")\n",
    "labels = labels.sort_values([\"id\",\"zone\"])\n",
    "labels = labels[\"Probability\"]\n",
    "labels = np.reshape(labels, (-1,17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fce2c15e290>, '_model_dir': '/output/multi-class8/multi_class', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fce2c15e290>, '_model_dir': '/output/multi-class8/multi_class', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'InputImagesIterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a0f3a5f2848e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZoneModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trimmed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"both\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors_to_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3f10f2c6cc7a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, tensors_to_log, reuse)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogging_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoggingTensorHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensors_to_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevery_n_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         tsa_classifier.fit(\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInputImagesIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInputLabelsIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'InputImagesIterator' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = ZoneModel(MODEL_ID, ids, \"trimmed\", \"both\", DATA_PATH, labels, CHECKPOINT_PATH, localize=False)     \n",
    "    model.train_model(tensors_to_log, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_header(infile):\n",
    "    \"\"\"Read image header (first 512 bytes)\n",
    "    \"\"\"\n",
    "    h = dict()\n",
    "    fid = open(infile, 'rb')\n",
    "    h['filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n",
    "    h['parent_filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n",
    "    h['comments1'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n",
    "    h['comments2'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n",
    "    h['energy_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['config_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['file_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['trans_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scan_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['data_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['date_modified'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 16))\n",
    "    h['frequency'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['mat_velocity'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['num_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_polarization_channels'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['spare00'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['adc_min_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['adc_max_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['band_width'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['spare01'] = np.fromfile(fid, dtype = np.int16, count = 5)\n",
    "    h['polarization_type'] = np.fromfile(fid, dtype = np.int16, count = 4)\n",
    "    h['record_header_size'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['word_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['word_precision'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['min_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['max_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['avg_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['data_scale_factor'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['data_units'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['surf_removal'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['edge_weighting'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['x_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['y_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['z_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['t_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['spare02'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['x_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['scan_orientation'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scan_direction'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['data_storage_order'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scanner_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['x_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['t_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['num_x_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_y_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_z_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_t_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['x_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['date_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n",
    "    h['time_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n",
    "    h['depth_recon'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['elevation_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['roll_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['azimuth_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['adc_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['spare06'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scanner_radius'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['t_delay'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['range_gate_start'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['range_gate_end'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['ahis_software_version'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['spare_end'] = np.fromfile(fid, dtype = np.float32, count = 10)\n",
    "    return h\n",
    "\n",
    "\n",
    "def read_data(infile, vertical=\"both\", horizontal=\"both\"):\n",
    "    \"\"\"Read any of the 4 types of image files, returns a numpy array of the image contents\n",
    "    \"\"\"\n",
    "    extension = os.path.splitext(infile)[1]\n",
    "    h = read_header(infile)\n",
    "    nx = int(h['num_x_pts'])\n",
    "    ny = int(h['num_y_pts'])\n",
    "    nt = int(h['num_t_pts'])\n",
    "    fid = open(infile, 'rb')\n",
    "    fid.seek(512) #skip header\n",
    "    if extension == '.aps' or extension == '.a3daps':\n",
    "        if(h['word_type']==7): #float32\n",
    "            data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n",
    "        elif(h['word_type']==4): #uint16\n",
    "            data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n",
    "        data = data * h['data_scale_factor'] #scaling factor\n",
    "        data = data.reshape(nx, ny, nt, order='F').copy() #make N-d image\n",
    "        if vertical == \"bottom\":\n",
    "            data = data[:, :340, :] \n",
    "        elif vertical == \"top\":\n",
    "            data = data[:, 320:660, :] \n",
    "        elif vertical == \"middle\":\n",
    "            data = data[:, 170:510, :] \n",
    "        rotated_data = []\n",
    "        if horizontal == \"right\":\n",
    "            for i in range(0,16):\n",
    "                increment = 10\n",
    "                rotated_data.append(data[(i * increment) + 50:(i * increment) + 320,:,i])\n",
    "        elif horizontal == \"left\":\n",
    "            for i in range(0,16):\n",
    "                increment = 10\n",
    "                rotated_data.append(data[210 - (i * increment): 480-(i * increment):,:,i])\n",
    "        elif horizontal == \"middle\":\n",
    "            for i in range(0,16):\n",
    "                increment = 10\n",
    "                rotated_data.append(data[130:400,:,i])\n",
    "        elif horizontal == \"trimmed\":\n",
    "            for i in range(0,16):\n",
    "                rotated_data.append(data[60:-60:,:,i])\n",
    "        else:\n",
    "            for i in range(0,16):\n",
    "                rotated_data.append(data[:,:,i])\n",
    "        data = np.array(rotated_data)  \n",
    "    fid.close()\n",
    "    return np.swapaxes(data, 2, 1)\n",
    "\n",
    "class InputImagesIterator:\n",
    "    def __init__(self, ids, data_path, contrast=1, vertical=\"both\", horizontal=\"both\", repeating=True):\n",
    "        self.ids=ids\n",
    "        self.contrast = contrast\n",
    "        self.data_path=data_path\n",
    "        self.i = -1\n",
    "        self.vertical = vertical\n",
    "        self.horizontal = horizontal\n",
    "        self.repeating = repeating\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "#         print (\"id \" + str(self.ids[self.i-1])) \n",
    "#         print(\"image iter \" + str(self.i))\n",
    "        if self.i < len(self.ids) -1:\n",
    "            self.i = self.i + 1\n",
    "            #print(\"IMAGES ITERATOR \" + str(self.ids[self.i - 1]))\n",
    "            return np.stack(read_data(self.data_path + self.ids[self.i] + \".aps\", self.vertical, self.horizontal) * self.contrast)\n",
    "        else:\n",
    "            if not self.repeating:\n",
    "                raise StopIteration()\n",
    "            #Restart iteration, cycle back through\n",
    "            self.i = -1\n",
    "            #print(\"IMAGES ITERATOR \" + str(self.ids[0]))\n",
    "            return np.stack(read_data(self.data_path + self.ids[0] + \".aps\", self.vertical, self.horizontal) * self.contrast)\n",
    "\n",
    "class InputLabelsIterator:\n",
    "    def __init__(self, ids, labels):\n",
    "        self.ids=ids\n",
    "        self.labels=labels\n",
    "        self.i=-1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "#         print(\"in label iter \" + str(self.ids[self.i -1])) \n",
    "        #print(\"in label iter \" + str(self.test_labels[self.i -1])) \n",
    "#         print(\"label iter \" + str(self.i))\n",
    "        if self.i < len(self.ids) -1:\n",
    "            self.i = self.i + 1\n",
    "            #print(\"LABEL\" + str(self.test_labels[self.i -1]))\n",
    "            #return(self.labels[self.i] + EPSILON)\n",
    "            return(self.labels[self.i])\n",
    "        else:\n",
    "            #Restart iteration, cycle back through\n",
    "            self.i = -1\n",
    "            #raise StopIteration()\n",
    "            #return(self.labels[0] + EPSILON)\n",
    "            return(self.labels[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/output/', 'multi_class')\n",
      "INFO:tensorflow:Using default config.\n",
      "Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fce2437bcd0>, '_model_dir': '/output//multi_class', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fce2437bcd0>, '_model_dir': '/output//multi_class', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "LOADED MODEL AT STEP 3727 FROM /output//multi_class\n",
      "LOADED MODEL <__main__.ZoneModel instance at 0x7fce7a7a8b48>\n",
      "WARNING:tensorflow:From <ipython-input-70-da9f637f76a5>:107: calling predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From <ipython-input-70-da9f637f76a5>:107: calling predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 660)\n",
      "Tensor(\"conv1/BiasAdd:0\", shape=(1, 16, 660, 392, 12), dtype=float32)\n",
      "Tensor(\"conv2/Relu:0\", shape=(1, 16, 330, 196, 12), dtype=float32)\n",
      "Tensor(\"pool1/MaxPool3D:0\", shape=(1, 1, 164, 97, 12), dtype=float32)\n",
      "Tensor(\"pool2/MaxPool:0\", shape=(1, 81, 48, 12), dtype=float32)\n",
      "Tensor(\"conv3/BiasAdd:0\", shape=(1, 81, 48, 12), dtype=float32)\n",
      "Tensor(\"pool1_2/MaxPool:0\", shape=(1, 80, 47, 12), dtype=float32)\n",
      "Tensor(\"conv4/Relu:0\", shape=(1, 80, 47, 12), dtype=float32)\n",
      "Tensor(\"pool2_2/MaxPool:0\", shape=(1, 80, 47, 12), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from /output//multi_class/model.ckpt-3727\n",
      "Restoring parameters from /output//multi_class/model.ckpt-3727\n"
     ]
    }
   ],
   "source": [
    "image_df = pd.read_csv(DATA_PATH + 'stage1_sample_submission.csv')\n",
    "image_df['zone'] = image_df['Id'].str.split(\"_\", expand=True)[1].str.strip()\n",
    "image_df['id'] = image_df['Id'].str.split(\"_\", expand=True)[0].str.strip()\n",
    "\n",
    "ids = image_df[\"id\"].unique()\n",
    "ids.sort()\n",
    "\n",
    "csv_file = open(CHECKPOINT_PATH + \"/submission.csv\", \"w+\")\n",
    "csv_file.write(\"Id,Probability\\n\")\n",
    "\n",
    "\n",
    "model = ZoneModel(MODEL_ID, ids, \"trimmed\",\"both\", DATA_PATH, image_df, '/output/', localize=False)\n",
    "model.load_model()\n",
    "print (\"LOADED MODEL \" + str(model))\n",
    "predicted = np.array([x[\"probabilities\"] for x in list(model.predict())])\n",
    "for i, image in enumerate(predicted):\n",
    "    for j, zone in enumerate(image):\n",
    "        csv_file.write(str(ids[i]) + \"_Zone\" + str(j+1) + \",\" + str(zone) + \"\\n\")\n",
    "csv_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p2.7]",
   "language": "python",
   "name": "conda-env-p2.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
