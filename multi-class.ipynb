{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import tensorflow as tf\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math \n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import tsa_utils\n",
    "import deep_cnn\n",
    "import mini_cnn\n",
    "\n",
    "from shutil import copytree\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import tensorflow as tf\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "import tsa_utils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import tensorflow as tf\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math \n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import tsa_utils\n",
    "import mini_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "BATCH_SIZE=10\n",
    "FILTER_COUNT=24\n",
    "KERNEL_SIZE1=(1,6,6)\n",
    "DEPTHSTRIDE=1\n",
    "XSTRIDE=1\n",
    "YSTRIDE=1\n",
    "POOLSIZE1=(16,3,3)\n",
    "POOLSIZE2=(3,3)\n",
    "POOL_STRIDES1=(1,2,2)\n",
    "POOL_STRIDES=(1,1)\n",
    "STEPS=3000\n",
    "#XSIZE=392\n",
    "XSIZE=512\n",
    "SIZE=660\n",
    "#XSIZE=270\n",
    "YSIZE=340\n",
    "LEARNING_RATE=0.001\n",
    "IMAGE_DEPTH=16\n",
    "CHANNELS=1\n",
    "FLAT_POOL_SIZE=57120\n",
    "\n",
    "DATA_PATH=\"./images/\"\n",
    "CHECKPOINT_PATH=\"/output/new_output2\"\n",
    "MODEL_ID=\"tsa_nb\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "slice_locations = {1:(\"top\",\"right\"),\n",
    "\t2:(\"middle\",\"right\"), \n",
    "\t3:(\"top\",\"left\"),\n",
    "\t4:(\"middle\",\"left\"),\n",
    "\t5:(\"top\",\"middle\"),\n",
    "\t6:(\"top\",\"right\"),\n",
    "\t7:(\"top\",\"left\"),\n",
    "\t8:(\"middle\",\"right\"),\n",
    "\t9:(\"middle\",\"middle\"),\n",
    "\t10:(\"middle\",\"left\"),\n",
    "\t11:(\"bottom\",\"right\"),\n",
    "\t12:(\"bottom\",\"left\"),\n",
    "\t13:(\"bottom\",\"right\"),\n",
    "\t14:(\"bottom\",\"left\"),\n",
    "\t15:(\"bottom\",\"right\"),\n",
    "\t16:(\"bottom\",\"left\"),\n",
    "\t17:(\"top\",\"middle\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ZoneModel():\n",
    "\n",
    "    def __init__(self, model_id, ids, x_slice, y_slice, data_path, labels, checkpoint_path=\".\", localize=False):\n",
    "        self.model_id = model_id\n",
    "        self.ids = ids\n",
    "        self.x_slice = x_slice\n",
    "        self.y_slice = y_slice\n",
    "        self.data_path = data_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.labels = labels\n",
    "        self.localize = localize\n",
    "\n",
    "    def build_model(self, data, labels, mode):\n",
    "        if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            BATCH_SIZE=1\n",
    "        else:\n",
    "            BATCH_SIZE=10\n",
    "        print(XSIZE, YSIZE)\n",
    "        data = tf.reshape(data, [BATCH_SIZE, IMAGE_DEPTH, YSIZE, XSIZE, CHANNELS])\n",
    "        conv1 = tf.layers.conv3d(inputs=data, filters=FILTER_COUNT, kernel_size=KERNEL_SIZE1, padding=\"same\", \n",
    "                strides=(DEPTHSTRIDE,XSTRIDE,YSTRIDE), name=\"conv1\", trainable=not self.localize)\n",
    "        print(conv1)\n",
    "        conv2 = tf.layers.conv3d(inputs=conv1, filters=FILTER_COUNT, kernel_size=(3,3,3), padding=\"same\",\n",
    "                strides=(1, 2, 1), name=\"conv2\", activation=tf.nn.relu, trainable=not self.localize)\n",
    "        print(conv2)\n",
    "        pool1 = tf.layers.max_pooling3d(inputs=conv2, pool_size=POOLSIZE1, strides=POOL_STRIDES1, name=\"pool1\")\n",
    "        print(pool1)\n",
    "        flattener = tf.reshape(pool1, [BATCH_SIZE, 84, 255, 24])\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=flattener, pool_size=(3,3), strides=(3,3), name=\"pool2\")\n",
    "        print(pool2)\n",
    "        conv3 = tf.layers.conv2d(inputs=pool2, filters=FILTER_COUNT, kernel_size=(3,3), padding=\"same\", \n",
    "                strides=(XSTRIDE,YSTRIDE), name=\"conv3\", trainable=not self.localize)\n",
    "        print(conv3)\n",
    "        pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=(2,2), strides=POOL_STRIDES, name=\"pool1\")\n",
    "        print(pool3)\n",
    "        conv4 = tf.layers.conv2d(inputs=pool3, filters=FILTER_COUNT, kernel_size=(3,3), padding=\"same\", \n",
    "                strides=(1, 1), name=\"conv4\", activation=tf.nn.relu, trainable=not self.localize)\n",
    "        print(conv4)\n",
    "        pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=POOLSIZE2, strides=POOL_STRIDES, name=\"pool2\", padding=\"same\")\n",
    "        print(pool4)\n",
    "        flat_pool = tf.reshape(pool2, [BATCH_SIZE, FLAT_POOL_SIZE])\n",
    "        flat_pool=tf.identity(flat_pool, name=\"flat_pool\")\n",
    "\n",
    "        logits = tf.layers.dense(inputs=flat_pool, units=17, trainable=True)\n",
    "        logits = tf.identity(logits, name=\"logits\")\n",
    "        logits = tf.reshape(logits, [BATCH_SIZE,17])\n",
    "        predictions = {\n",
    "            \"classes\": tf.argmax(\n",
    "              input=logits, axis=1, name=\"classes\"),\n",
    "          \"probabilities\": tf.nn.softmax(\n",
    "              logits, name=\"softmax_tensor\")}\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            #flat_labels = tf.reshape(labels, [BATCH_SIZE, 17])\n",
    "            labels=tf.identity(labels, name=\"labels\")\n",
    "            #class_weights=tf.reduce_sum(tf.multiply(flat_labels, tf.constant(WEIGHTS, dtype=tf.int64)), axis=1)\n",
    "            #print(class_weights)\n",
    "            print(labels)\n",
    "            print(logits)\n",
    "            loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=tf.contrib.framework.get_global_step(),\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                optimizer=\"SGD\")\n",
    "        if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            return tf.contrib.learn.ModelFnOps(mode=mode, predictions=predictions)\n",
    "        return tf.contrib.learn.ModelFnOps(mode=mode, predictions=predictions, loss=loss, train_op=train_op)\n",
    "\n",
    "    def train_model(self, tensors_to_log, reuse=False):\n",
    "        if reuse:\n",
    "            tsa_classifier = self.model\n",
    "        else:\n",
    "            tsa_classifier = tf.contrib.learn.Estimator(model_fn=self.build_model, \n",
    "                                                     model_dir=self.checkpoint_path + \"/\" + self.model_id)\n",
    "        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=100)\n",
    "        tsa_classifier.fit(\n",
    "            x=InputImagesIterator(self.ids, self.data_path, 10000, self.y_slice, self.x_slice), \n",
    "            y=InputLabelsIterator(self.ids, self.labels), \n",
    "            steps=STEPS, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            monitors=[logging_hook])\n",
    "        self.model = tsa_classifier\n",
    "\n",
    "    def load_model(self):\n",
    "        tsa_classifier = tf.contrib.learn.Estimator(model_fn=self.build_model, \n",
    "                                                    model_dir=self.checkpoint_path + \"/\" + self.model_id)\n",
    "        print (\"LOADED MODEL AT STEP \" + str(tsa_classifier.get_variable_value(\"global_step\")) + \" FROM \" + self.checkpoint_path + \"/\" + self.model_id )\n",
    "        self.model = tsa_classifier\n",
    "\n",
    "    def bootstrap_model(self, path=None):\n",
    "        if path:\n",
    "            checkpoint_path = path\n",
    "        else:\n",
    "            checkpoint_path = self.checkpoint_path + \"/\" + self.model_id\n",
    "        print (\"USING CHECKPOINT PATH  \" + checkpoint_path)\n",
    "        tsa_classifier = tf.contrib.learn.Estimator(model_fn=self.build_model, \n",
    "                                                    model_dir=checkpoint_path)\n",
    "        print (\"BOOTSTRAPPED AT STEP \" + str(tsa_classifier.get_variable_value(\"global_step\")))\n",
    "        self.model = tsa_classifier \n",
    "\n",
    "    def predict(self):\n",
    "        return self.model.predict(x=InputImagesIterator(self.ids, \n",
    "                                                            self.data_path, \n",
    "                                                            10000, \n",
    "                                                            self.y_slice,\n",
    "                                                            self.x_slice, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = pd.read_csv(DATA_PATH + '/stage1_labels.csv')\n",
    "image_df['zone'] = image_df['Id'].str.split(\"_Zone\", expand=True)[1].str.strip()\n",
    "image_df['id'] = image_df['Id'].str.split(\"_\", expand=True)[0].str.strip()\n",
    "\n",
    "ids = image_df[\"id\"].unique()\n",
    "ids.sort()\n",
    "training_ids = ids\n",
    "\n",
    "labels = image_df[image_df['id'].isin(training_ids)]\n",
    "labels = labels.sort_values(\"id\")\n",
    "\n",
    "tensors_to_log =  {\"probabilities\": \"softmax_tensor\",\n",
    "                    \"actual\":\"labels\"}\n",
    "labels.zone = labels.zone.astype(\"int\")\n",
    "labels.sort_values([\"id\",\"zone\"])\n",
    "labels = labels[\"Probability\"]\n",
    "labels = np.reshape(labels, (-1,17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb9e7a90>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:From <ipython-input-82-0d187a437bbf>:82: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-82-0d187a437bbf>:82: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-82-0d187a437bbf>:82: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "(16, 340, 512)\n",
      "(512, 340)\n",
      "Tensor(\"conv1/BiasAdd:0\", shape=(10, 16, 340, 512, 24), dtype=float32)\n",
      "Tensor(\"conv2/Relu:0\", shape=(10, 16, 170, 512, 24), dtype=float32)\n",
      "Tensor(\"pool1/MaxPool3D:0\", shape=(10, 1, 84, 255, 24), dtype=float32)\n",
      "Tensor(\"pool2/MaxPool:0\", shape=(10, 28, 85, 24), dtype=float32)\n",
      "Tensor(\"conv3/BiasAdd:0\", shape=(10, 28, 85, 24), dtype=float32)\n",
      "Tensor(\"pool1_2/MaxPool:0\", shape=(10, 27, 84, 24), dtype=float32)\n",
      "Tensor(\"conv4/Relu:0\", shape=(10, 27, 84, 24), dtype=float32)\n",
      "Tensor(\"pool2_2/MaxPool:0\", shape=(10, 27, 84, 24), dtype=float32)\n",
      "Tensor(\"labels:0\", shape=(?, 17), dtype=int64)\n",
      "Tensor(\"Reshape_3:0\", shape=(10, 17), dtype=float32)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "(16, 340, 512)\n",
      "(16, 340, 512)\n",
      "(16, 340, 512)\n",
      "(16, 340, 512)\n",
      "(16, 340, 512)\n",
      "(16, 340, 512)\n",
      "(16, 340, 512)\n",
      "(16, 340, 512)\n",
      "(16, 340, 512)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = ZoneModel(MODEL_ID, training_ids, \"trimmed\", \"bottom\", DATA_PATH, labels, CHECKPOINT_PATH, localize=False)     \n",
    "    model.train_model(tensors_to_log, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_header(infile):\n",
    "    \"\"\"Read image header (first 512 bytes)\n",
    "    \"\"\"\n",
    "    h = dict()\n",
    "    fid = open(infile, 'rb')\n",
    "    h['filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n",
    "    h['parent_filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n",
    "    h['comments1'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n",
    "    h['comments2'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n",
    "    h['energy_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['config_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['file_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['trans_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scan_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['data_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['date_modified'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 16))\n",
    "    h['frequency'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['mat_velocity'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['num_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_polarization_channels'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['spare00'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['adc_min_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['adc_max_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['band_width'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['spare01'] = np.fromfile(fid, dtype = np.int16, count = 5)\n",
    "    h['polarization_type'] = np.fromfile(fid, dtype = np.int16, count = 4)\n",
    "    h['record_header_size'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['word_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['word_precision'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['min_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['max_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['avg_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['data_scale_factor'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['data_units'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['surf_removal'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['edge_weighting'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['x_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['y_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['z_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['t_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['spare02'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['x_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['scan_orientation'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scan_direction'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['data_storage_order'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scanner_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['x_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['t_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['num_x_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_y_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_z_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_t_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['x_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['date_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n",
    "    h['time_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n",
    "    h['depth_recon'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['elevation_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['roll_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['azimuth_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['adc_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['spare06'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scanner_radius'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['t_delay'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['range_gate_start'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['range_gate_end'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['ahis_software_version'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['spare_end'] = np.fromfile(fid, dtype = np.float32, count = 10)\n",
    "    return h\n",
    "\n",
    "\n",
    "def read_data(infile, vertical=\"both\", horizontal=\"both\"):\n",
    "    \"\"\"Read any of the 4 types of image files, returns a numpy array of the image contents\n",
    "    \"\"\"\n",
    "    extension = os.path.splitext(infile)[1]\n",
    "    h = read_header(infile)\n",
    "    nx = int(h['num_x_pts'])\n",
    "    ny = int(h['num_y_pts'])\n",
    "    nt = int(h['num_t_pts'])\n",
    "    fid = open(infile, 'rb')\n",
    "    fid.seek(512) #skip header\n",
    "    if extension == '.aps' or extension == '.a3daps':\n",
    "        if(h['word_type']==7): #float32\n",
    "            data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n",
    "        elif(h['word_type']==4): #uint16\n",
    "            data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n",
    "        data = data * h['data_scale_factor'] #scaling factor\n",
    "        data = data.reshape(nx, ny, nt, order='F').copy() #make N-d image\n",
    "        if vertical == \"bottom\":\n",
    "            data = data[:, :340, :] \n",
    "        elif vertical == \"top\":\n",
    "            data = data[:, 320:660, :] \n",
    "        elif vertical == \"middle\":\n",
    "            data = data[:, 170:510, :] \n",
    "        rotated_data = []\n",
    "        if horizontal == \"right\":\n",
    "            for i in range(0,16):\n",
    "                increment = 10\n",
    "                rotated_data.append(data[(i * increment) + 50:(i * increment) + 320,:,i])\n",
    "        elif horizontal == \"left\":\n",
    "            for i in range(0,16):\n",
    "                increment = 10\n",
    "                rotated_data.append(data[210 - (i * increment): 480-(i * increment):,:,i])\n",
    "        elif horizontal == \"middle\":\n",
    "            for i in range(0,16):\n",
    "                increment = 10\n",
    "                rotated_data.append(data[130:400,:,i])\n",
    "        else:\n",
    "            for i in range(0,16):\n",
    "                rotated_data.append(data[:,:,i])\n",
    "        data = np.array(rotated_data)  \n",
    "    elif extension == '.a3d':\n",
    "        if(h['word_type']==7): #float32\n",
    "            data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n",
    "        elif(h['word_type']==4): #uint16\n",
    "            data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n",
    "        data = data * h['data_scale_factor'] #scaling factor\n",
    "        data = data.reshape(nx, nt, ny, order='F').copy() #make N-d image\n",
    "    elif extension == '.ahi':\n",
    "        data = np.fromfile(fid, dtype = np.float32, count = 2* nx * ny * nt)\n",
    "        data = data.reshape(2, ny, nx, nt, order='F').copy()\n",
    "        real = data[0,:,:,:].copy()\n",
    "        imag = data[1,:,:,:].copy()\n",
    "    fid.close()\n",
    "    if extension != '.ahi':\n",
    "        return np.swapaxes(data, 2, 1)\n",
    "    else:\n",
    "        return real, imag\n",
    "\n",
    "class InputImagesIterator:\n",
    "    def __init__(self, ids, data_path, contrast=1, vertical=\"both\", horizontal=\"both\", repeating=True):\n",
    "        self.ids=ids\n",
    "        self.contrast = contrast\n",
    "        self.data_path=data_path\n",
    "        self.i = -1\n",
    "        self.vertical = vertical\n",
    "        self.horizontal = horizontal\n",
    "        self.repeating = repeating\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        if self.i < len(self.ids) -1:\n",
    "            self.i = self.i + 1\n",
    "            print(np.stack(read_data(self.data_path + self.ids[self.i] + \".aps\", self.vertical, self.horizontal) * self.contrast).shape)\n",
    "            return np.stack(read_data(self.data_path + self.ids[self.i] + \".aps\", self.vertical, self.horizontal) * self.contrast)            \n",
    "        else:\n",
    "            if not self.repeating:\n",
    "                raise StopIteration()\n",
    "            #Restart iteration, cycle back through\n",
    "            self.i = -1\n",
    "            return np.stack(read_data(self.data_path + self.ids[0] + \".aps\", self.vertical, self.horizontal) * self.contrast)\n",
    "\n",
    "class InputLabelsIterator:\n",
    "    def __init__(self, ids, labels):\n",
    "        self.ids=ids\n",
    "        self.labels=labels\n",
    "        self.i=-1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        #print(\"in label iter \" + str(self.ids[self.i -1])) \n",
    "        #print(\"in label iter \" + str(self.test_labels[self.i -1])) \n",
    "        #print(\"label iter \" + str(self.i))\n",
    "        if self.i < len(self.ids) -1:\n",
    "            self.i = self.i + 1\n",
    "            #print(\"LABEL\" + str(self.test_labels[self.i -1]))\n",
    "            return(self.labels[self.i])\n",
    "        else:\n",
    "            #Restart iteration, cycle back through\n",
    "            self.i = -1\n",
    "            #raise StopIteration()\n",
    "            return(self.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p2.7]",
   "language": "python",
   "name": "conda-env-p2.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
